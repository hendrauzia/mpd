{
 "metadata": {
  "name": "fiddle",
  "kernelspec": {
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, col, countDistinct, explode, lit, lower, max, regexp_replace, row_number, sqrt, trim, udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType, LongType, MapType, StringType, StructField, StructType\n",
    "\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class Playlists:\n",
    "  def __init__(self):\n",
    "    self.extract()\n",
    "    self.transform()\n",
    "\n",
    "  def extract(self):\n",
    "    self.playlists = (spark\n",
    "        .read\n",
    "        .option(\"multiline\",\"true\")\n",
    "        .json(\"data/challenge_set.json\"))\n",
    "    \n",
    "  def transform(self):\n",
    "    playlists = (self.playlists\n",
    "      .withColumn(\"playlist\", explode(\"playlists\"))\n",
    "      .select(\n",
    "          col(\"playlist.pid\").alias(\"playlist_id\"),\n",
    "          regexp_replace(trim(lower(col(\"playlist.name\"))), r\"\\s{2,}\", \" \").alias(\"playlist_name\"),\n",
    "          col(\"playlist.tracks\").alias(\"tracks\")))\n",
    "    \n",
    "    self.dataframe = playlists\n",
    "\n",
    "playlists = Playlists()\n",
    "playlists.dataframe.printSchema()\n",
    "\n",
    "z.show(playlists.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class Tracks:\n",
    "    def __init__(self, playlists):\n",
    "        self.playlists = playlists\n",
    "        self.transform()\n",
    "        \n",
    "    def transform(self):\n",
    "        tracks = (self.playlists\n",
    "            .withColumn(\"track\", explode(\"tracks\"))\n",
    "            .select(\n",
    "                col(\"playlist_id\"),\n",
    "                col(\"playlist_name\"),\n",
    "                col(\"track.artist_uri\").alias(\"artist_uri\"),\n",
    "                col(\"track.artist_name\").alias(\"artist_name\"),\n",
    "                col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "                col(\"track.track_name\").alias(\"track_name\")))\n",
    "        \n",
    "        self.dataframe = tracks\n",
    "        \n",
    "tracks = Tracks(playlists.dataframe)\n",
    "tracks.dataframe.printSchema()\n",
    "\n",
    "z.show(tracks.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class PlaylistPreprocessing:\n",
    "    def __init__(self, playlists):\n",
    "        self.playlists = playlists\n",
    "        self.transform()\n",
    "    \n",
    "    def transform(self):\n",
    "        playlists = (self.playlists\n",
    "            .select(\n",
    "                col(\"playlist_name\"))\n",
    "            .where(\n",
    "                (col(\"playlist_name\").isNotNull()) &\n",
    "                (col(\"playlist_name\") != \"\"))\n",
    "            .distinct())\n",
    "\n",
    "        self.dataframe = playlists\n",
    "\n",
    "playlist_preprocessing = PlaylistPreprocessing(playlists.dataframe)\n",
    "playlist_preprocessing.dataframe.printSchema()\n",
    "\n",
    "z.show(playlist_preprocessing.dataframe.count())\n",
    "z.show(playlist_preprocessing.dataframe.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class TrackPreprocessing:\n",
    "    def __init__(self, tracks):\n",
    "        self.tracks = tracks\n",
    "        self.transform()\n",
    "    \n",
    "    def transform(self):\n",
    "        tracks = (self.tracks\n",
    "            .where(\n",
    "                (col(\"playlist_name\").isNotNull()) &\n",
    "                (col(\"playlist_name\") != \"\"))\n",
    "            .distinct())\n",
    "\n",
    "        self.dataframe = tracks\n",
    "\n",
    "track_preprocessing = TrackPreprocessing(tracks.dataframe)\n",
    "track_preprocessing.dataframe.printSchema()\n",
    "\n",
    "z.show(track_preprocessing.dataframe.count())\n",
    "z.show(track_preprocessing.dataframe.limit(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class Dataset:\n",
    "  def __init__(self, dataframe, fold=0, tracks=None):\n",
    "    self.dataframe = dataframe\n",
    "    self.seed = fold * 1000\n",
    "    self.tracks = tracks\n",
    "    \n",
    "    self.split()\n",
    "\n",
    "  def split(self):\n",
    "    self.train, self.test, self.validation = self.dataframe.randomSplit([0.6, 0.2, 0.2], self.seed)\n",
    "    \n",
    "    if self.tracks:\n",
    "        self.validation = (self.validation\n",
    "            .join(self.tracks, [\"playlist_name\"]))\n",
    "\n",
    "playlist_dataset = Dataset(playlist_preprocessing.dataframe, tracks=tracks.dataframe)\n",
    "\n",
    "z.show(playlist_dataset.train.limit(1))\n",
    "z.show(playlist_dataset.test.limit(1))\n",
    "z.show(playlist_dataset.validation.limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "track_dataset = Dataset(tracks.dataframe)\n",
    "\n",
    "z.show(track_dataset.train.limit(2))\n",
    "z.show(track_dataset.test.limit(2))\n",
    "z.show(track_dataset.validation.limit(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class Random:\n",
    "    def __init__(self, tracks, _=None):\n",
    "        self.tracks = tracks\n",
    "        self.transform()\n",
    "        \n",
    "    def transform(self):\n",
    "        tracks = (self.tracks\n",
    "            .select(\n",
    "                col(\"track_uri\").alias(\"recommendation_track_uri\"))\n",
    "            .distinct())\n",
    "        \n",
    "        tracks = (tracks\n",
    "            .sample(fraction=500/tracks.count(), seed=0)\n",
    "            .limit(500))\n",
    "            \n",
    "        self.dataframe = tracks\n",
    "\n",
    "random = Random(track_dataset.train)\n",
    "\n",
    "z.show(random.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class RandomRecommendations:\n",
    "  def __init__(self, test, random, _=None, __=None):\n",
    "    self.test = test\n",
    "    self.random = random\n",
    "    self.transform()\n",
    "\n",
    "  def transform(self):\n",
    "    random = (self.random\n",
    "      .select(\n",
    "        col(\"recommendation_track_uri\")))\n",
    "\n",
    "    recommendations = (self.test\n",
    "      .join(random))\n",
    "\n",
    "    self.dataframe = recommendations\n",
    "    \n",
    "random_recommendations = RandomRecommendations(track_dataset.test, random.dataframe)\n",
    "\n",
    "z.show(random_recommendations.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, validation, recommendations, key=\"playlist_id\", fold=None):\n",
    "        self.validation = validation\n",
    "        self.recommendations = recommendations\n",
    "        self.key = key\n",
    "        self.fold = fold\n",
    "        \n",
    "        self.evaluate()\n",
    "        \n",
    "    def evaluate(self):\n",
    "        playlists = (self.validation\n",
    "            .groupBy(self.key)\n",
    "            .agg(\n",
    "                countDistinct(\"track_uri\").alias(\"tracks\")))\n",
    "        \n",
    "        tracks = (self.recommendations\n",
    "            .select(\n",
    "                col(\"recommendation_track_uri\")))\n",
    "\n",
    "        tracks_hits = (self.validation\n",
    "            .join(tracks, self.validation[\"track_uri\"] == tracks[\"recommendation_track_uri\"])\n",
    "            .groupBy(self.key)\n",
    "            .agg(\n",
    "                countDistinct(\"track_uri\").alias(\"tracks_hits\")))\n",
    "                \n",
    "        evaluation = (playlists\n",
    "            .join(tracks_hits, [self.key], \"left\")\n",
    "            .withColumn(\"R-precision\", col(\"tracks_hits\") / col(\"tracks\"))\n",
    "            .agg(\n",
    "                avg(\"R-precision\").alias(\"R-precision\")))\n",
    "                \n",
    "        if self.fold:\n",
    "            evaluation = (evaluation\n",
    "                .withColumn(\"fold\", lit(self.fold)))\n",
    "        \n",
    "        self.dataframe = evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "random_evaluation = Evaluation(track_dataset.validation, random_recommendations.dataframe)\n",
    "\n",
    "z.show(random_evaluation.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def embeddings(text):\n",
    "    return nlp(text).vector.tolist()\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "to_cosine_similarity = udf(cosine_similarity, DoubleType())\n",
    "to_embeddings = udf(embeddings, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class PlaylistEmbeddings:\n",
    "    def __init__(self, playlists):\n",
    "        self.playlists = playlists\n",
    "        self.transform()\n",
    "        \n",
    "    def transform(self):\n",
    "        playlists = (self.playlists\n",
    "            .withColumn(\"playlist_embedding\", to_embeddings(col(\"playlist_name\"))))\n",
    "            \n",
    "        self.dataframe = playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sudo su\n",
    "source /opt/conda/miniconda3/bin/activate\n",
    "\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "embeddings = PlaylistEmbeddings(playlist_dataset.train)\n",
    "embeddings.dataframe.printSchema()\n",
    "\n",
    "z.show(embeddings.dataframe.count())\n",
    "z.show(embeddings.dataframe.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class PlaylistSimilarities:\n",
    "    def __init__(self, playlists_embeddings):\n",
    "        self.playlists = playlists_embeddings\n",
    "        self.transform()\n",
    "        \n",
    "    def transform(self):\n",
    "        playlists = (self.playlists.alias(\"playlists\")\n",
    "            .join(self.playlists.alias(\"recommendations\"),\n",
    "                col(\"playlists.playlist_name\") < col(\"recommendations.playlist_name\"))\n",
    "            .withColumn(\"similarity\",\n",
    "                to_cosine_similarity(\n",
    "                    col(\"playlists.playlist_embedding\"),\n",
    "                    col(\"recommendations.playlist_embedding\")))\n",
    "            .where(\n",
    "                (col(\"similarity\") > 0.5))\n",
    "            .select(\n",
    "                col(\"playlists.playlist_name\"),\n",
    "                col(\"recommendations.playlist_name\").alias(\"recommendation_playlist_name\"),\n",
    "                col(\"similarity\")))\n",
    "                \n",
    "        self.dataframe = playlists\n",
    "\n",
    "playlist_similarities = PlaylistSimilarities(embeddings.dataframe)\n",
    "playlist_similarities.dataframe.printSchema()\n",
    "\n",
    "z.show(playlist_similarities.dataframe.count())\n",
    "z.show(playlist_similarities.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class PlaylistSimilaritiesWithTracks:\n",
    "    def __init__(self, playlist_similarities, tracks):\n",
    "        self.similarities = playlist_similarities\n",
    "        self.tracks = tracks\n",
    "        self.transform()\n",
    "        \n",
    "    def transform(self):\n",
    "        tracks = (self.tracks\n",
    "            .select(\n",
    "                col(\"playlist_name\").alias(\"recommendation_playlist_name\"),\n",
    "                col(\"artist_uri\").alias(\"recommendation_artist_uri\"),\n",
    "                col(\"artist_name\").alias(\"recommendation_artist_name\"),\n",
    "                col(\"track_uri\").alias(\"recommendation_track_uri\"),\n",
    "                col(\"track_name\").alias(\"recommendation_track_name\")))\n",
    "        \n",
    "        window = Window.partitionBy(col(\"playlist_name\")).orderBy(col(\"similarity\").desc())\n",
    "        similarities = (self.similarities\n",
    "            .join(tracks, [\"recommendation_playlist_name\"])\n",
    "            .groupBy(\n",
    "                col(\"playlist_name\"),\n",
    "                col(\"recommendation_artist_uri\"),\n",
    "                col(\"recommendation_artist_name\"),\n",
    "                col(\"recommendation_track_uri\"),\n",
    "                col(\"recommendation_track_name\"))\n",
    "            .agg(avg(\"similarity\").alias(\"similarity\"))\n",
    "            .withColumn(\"position\", row_number().over(window))\n",
    "            .where(col(\"position\") <= 500))\n",
    "\n",
    "        self.dataframe = similarities\n",
    "        \n",
    "playlist_similarities_with_tracks = PlaylistSimilaritiesWithTracks(\n",
    "    playlist_similarities.dataframe,\n",
    "    tracks.dataframe)\n",
    "playlist_similarities_with_tracks.dataframe.printSchema()\n",
    "\n",
    "z.show(playlist_similarities_with_tracks.dataframe.count())\n",
    "z.show(playlist_similarities_with_tracks.dataframe.limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class PlaylistSimilaritiesWrapper:\n",
    "    def __init__(self, playlists, tracks):\n",
    "        self.playlists = playlists\n",
    "        self.tracks = tracks\n",
    "        \n",
    "        self.transform()\n",
    "        \n",
    "    def transform(self):\n",
    "        embeddings = PlaylistEmbeddings(self.playlists)\n",
    "        similarities = PlaylistSimilarities(embeddings.dataframe)\n",
    "        similarities_with_tracks = PlaylistSimilaritiesWithTracks(similarities.dataframe, tracks.dataframe)\n",
    "\n",
    "        self.dataframe = similarities_with_tracks.dataframe\n",
    "        \n",
    "playlist_similarities_wrapper = PlaylistSimilaritiesWrapper(playlist_dataset.train, tracks.dataframe)\n",
    "playlist_similarities_wrapper.dataframe.printSchema()\n",
    "\n",
    "z.show(playlist_similarities_wrapper.dataframe.count())\n",
    "z.show(playlist_similarities_wrapper.dataframe.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class Recommendations:\n",
    "  def __init__(self, test, similarities, key=\"playlist_id\", interaction=\"track_uri\", total=500):\n",
    "    self.test = test\n",
    "    self.similarities = similarities\n",
    "    self.key = key\n",
    "    self.interaction = interaction\n",
    "    self.total = total\n",
    "    \n",
    "    self.transform()\n",
    "\n",
    "  def transform(self):\n",
    "    similarities = (self.similarities\n",
    "      .select(\n",
    "        col(self.interaction),\n",
    "        col(\"recommendation_artist_uri\"),\n",
    "        col(\"recommendation_track_uri\"),\n",
    "        col(\"similarity\")))\n",
    "    \n",
    "    window = (Window\n",
    "      .partitionBy(\n",
    "        col(self.key))\n",
    "      .orderBy(\n",
    "        col(\"similarity\").desc()))\n",
    "\n",
    "    recommendations = (self.test\n",
    "      .join(similarities, [self.interaction])\n",
    "      .withColumn(\"position\", row_number().over(window))\n",
    "      .where(\n",
    "        col(\"position\") <= self.total))\n",
    "\n",
    "    recommendations = (recommendations\n",
    "      .select(sorted(recommendations.columns)))\n",
    "\n",
    "    self.dataframe = recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "playlist_recommendations = Recommendations(\n",
    "    playlist_dataset.test,\n",
    "    playlist_similarities_wrapper.dataframe,\n",
    "    key=\"playlist_name\",\n",
    "    interaction=\"playlist_name\")\n",
    "    \n",
    "playlist_recommendations.dataframe.printSchema()\n",
    "z.show(playlist_recommendations.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "playlist_evaluation = Evaluation(\n",
    "    playlist_dataset.validation, \n",
    "    playlist_recommendations.dataframe, \n",
    "    key=\"playlist_name\")\n",
    "\n",
    "z.show(playlist_evaluation.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class TrackSimilarities:\n",
    "  def __init__(self, dataframe, _=None, similarity=\"jaccard\"):\n",
    "    self.tracks = dataframe\n",
    "    self.similarity = similarity\n",
    "    \n",
    "    self.transform()\n",
    "\n",
    "  def transform(self):\n",
    "    tracks = self.tracks\n",
    "\n",
    "    similarities = (tracks.alias(\"tracks\")\n",
    "      .join(tracks.alias(\"recommendations\"),\n",
    "        col(\"tracks.track_uri\") != col(\"recommendations.track_uri\"))\n",
    "      .where(col(\"tracks.playlist_id\") == col(\"recommendations.playlist_id\"))\n",
    "      .select(\n",
    "        col(\"tracks.artist_uri\"),\n",
    "        col(\"tracks.artist_name\"),\n",
    "        col(\"tracks.track_uri\"),\n",
    "        col(\"tracks.track_name\"),\n",
    "        col(\"tracks.playlist_id\"),\n",
    "        col(\"recommendations.artist_uri\").alias(\"recommendation_artist_uri\"),\n",
    "        col(\"recommendations.artist_name\").alias(\"recommendation_artist_name\"),\n",
    "        col(\"recommendations.track_uri\").alias(\"recommendation_track_uri\"),\n",
    "        col(\"recommendations.track_name\").alias(\"recommendation_track_name\"))\n",
    "      .groupBy(\n",
    "        col(\"tracks.artist_uri\"),\n",
    "        col(\"tracks.artist_name\"),\n",
    "        col(\"tracks.track_uri\"),\n",
    "        col(\"tracks.track_name\"),\n",
    "        col(\"recommendation_artist_uri\"),\n",
    "        col(\"recommendation_artist_name\"),\n",
    "        col(\"recommendation_track_uri\"),\n",
    "        col(\"recommendation_track_name\"))\n",
    "      .agg(\n",
    "        countDistinct(\"playlist_id\").alias(\"intersection\")))\n",
    "\n",
    "    playlists = (tracks\n",
    "      .groupBy(\"track_uri\")\n",
    "      .agg(\n",
    "        countDistinct(\"playlist_id\").alias(\"playlists\")))\n",
    "\n",
    "    window_jaccard = (Window\n",
    "      .partitionBy(col(\"tracks.track_uri\"))\n",
    "      .orderBy(col(\"similarity_jaccard\").desc()))\n",
    "    \n",
    "    window_cosine = (Window\n",
    "      .partitionBy(col(\"tracks.track_uri\"))\n",
    "      .orderBy(col(\"similarity_cosine\").desc()))\n",
    "      \n",
    "    similarities = (similarities\n",
    "      .join(playlists.alias(\"a\"), [\"track_uri\"])\n",
    "      .join(playlists.alias(\"b\"), col(\"recommendation_track_uri\") == col(\"b.track_uri\"))\n",
    "      .withColumn(\"union\",\n",
    "        col(\"a.playlists\") + col(\"b.playlists\") - col(\"intersection\"))\n",
    "      .withColumn(\"similarity_jaccard\",\n",
    "        col(\"intersection\") / col(\"union\"))\n",
    "      .withColumn(\"similarity_cosine\",\n",
    "        col(\"intersection\") / (sqrt(col(\"a.playlists\")) * sqrt(col(\"b.playlists\"))))\n",
    "      .withColumn(\"similarity\", col(f\"similarity_{self.similarity}\"))\n",
    "      .withColumn(\"position_jaccard\", row_number().over(window_jaccard))\n",
    "      .withColumn(\"position_cosine\", row_number().over(window_cosine))\n",
    "      .withColumn(\"position\", col(f\"position_{self.similarity}\"))\n",
    "      .select(\n",
    "        col(\"tracks.artist_uri\"),\n",
    "        col(\"tracks.artist_name\"),\n",
    "        col(\"tracks.track_uri\"),\n",
    "        col(\"tracks.track_name\"),\n",
    "        col(\"a.playlists\").alias(\"track_playlists\"),\n",
    "        col(\"recommendation_artist_uri\"),\n",
    "        col(\"recommendation_artist_name\"),\n",
    "        col(\"recommendation_track_uri\"),\n",
    "        col(\"recommendation_track_name\"),\n",
    "        col(\"b.playlists\").alias(\"recommendation_track_playlists\"),\n",
    "        col(\"intersection\"),\n",
    "        col(\"union\"),\n",
    "        col(\"similarity_jaccard\"),\n",
    "        col(\"similarity_cosine\"),\n",
    "        col(\"similarity\"),\n",
    "        col(\"position_jaccard\"),\n",
    "        col(\"position_cosine\"),\n",
    "        col(\"position\"))\n",
    "      .where(col(\"position\") <= 500))\n",
    "\n",
    "    self.dataframe = similarities\n",
    "    \n",
    "track_similarities = TrackSimilarities(track_dataset.train)\n",
    "track_similarities.dataframe.printSchema()\n",
    "\n",
    "z.show(track_similarities.dataframe.count())\n",
    "z.show(track_similarities.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "recommendations = Recommendations(track_dataset.test, track_similarities.dataframe)\n",
    "recommendations.dataframe.printSchema()\n",
    "\n",
    "z.show(recommendations.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "        \n",
    "evaluation = Evaluation(track_dataset.validation, recommendations.dataframe)\n",
    "\n",
    "z.show(evaluation.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class TrackSimilaritiesCosine(TrackSimilarities):\n",
    "  def __init__(self, dataframe, _=None, similarity=\"cosine\"):\n",
    "    self.tracks = dataframe\n",
    "    self.similarity = similarity\n",
    "    \n",
    "    self.transform()\n",
    "\n",
    "track_similarities_cosine = TrackSimilaritiesCosine(\n",
    "    track_dataset.train, \n",
    "    similarity=\"cosine\")\n",
    "\n",
    "track_similarities_cosine.dataframe.printSchema()\n",
    "\n",
    "z.show(track_similarities_cosine.dataframe.count())\n",
    "z.show(track_similarities_cosine.dataframe.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "recommendations_cosine = Recommendations(\n",
    "    track_dataset.test,\n",
    "    track_similarities_cosine.dataframe)\n",
    "\n",
    "recommendations_cosine.dataframe.printSchema()\n",
    "\n",
    "z.show(recommendations_cosine.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "        \n",
    "evaluation_cosine = Evaluation(\n",
    "    track_dataset.validation, \n",
    "    recommendations_cosine.dataframe)\n",
    "\n",
    "z.show(evaluation_cosine.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "hybrid_dataset = Dataset(track_preprocessing.dataframe)\n",
    "\n",
    "z.show(hybrid_dataset.train.limit(5))\n",
    "z.show(hybrid_dataset.test.limit(5))\n",
    "z.show(hybrid_dataset.validation.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "class HybridRecommendations:\n",
    "    def __init__(self, test, playlist_similarities, track_similarities, weights):\n",
    "        self.test = test\n",
    "        self.playlist_similarities = playlist_similarities\n",
    "        self.track_similarities = track_similarities\n",
    "        \n",
    "        self.playlist_weight, self.track_weight = weights\n",
    "        self.total = 500\n",
    "\n",
    "        self.transform()\n",
    "        \n",
    "    def transform(self):\n",
    "        playlist_recommendations = Recommendations(\n",
    "            self.test,\n",
    "            self.playlist_similarities,\n",
    "            key=\"playlist_name\",\n",
    "            interaction=\"playlist_name\",\n",
    "            total=(self.playlist_weight * self.total))\n",
    "            \n",
    "        track_recommendations = Recommendations(\n",
    "            self.test,\n",
    "            self.track_similarities,\n",
    "            key=\"playlist_id\",\n",
    "            interaction=\"track_uri\",\n",
    "            total=(self.track_weight * self.total))\n",
    "            \n",
    "        self.dataframe = playlist_recommendations.dataframe.union(track_recommendations.dataframe)\n",
    "\n",
    "hybrid_recommendations = HybridRecommendations(\n",
    "    hybrid_dataset.test, \n",
    "    playlist_similarities_wrapper.dataframe,\n",
    "    track_similarities.dataframe,\n",
    "    [0.5, 0.5])\n",
    "\n",
    "hybrid_recommendations.dataframe.printSchema()\n",
    "z.show(hybrid_recommendations.dataframe.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "        \n",
    "hybrid_evaluation = Evaluation(hybrid_dataset.validation, hybrid_recommendations.dataframe)\n",
    "\n",
    "z.show(hybrid_evaluation.dataframe.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class CrossValidation:\n",
    "    def __init__(self, input, folds, train_builder, test_builder, key, interaction=None, tracks=None):\n",
    "        self.input = input\n",
    "        self.folds = folds\n",
    "        self.train_builder = train_builder\n",
    "        self.test_builder = test_builder\n",
    "        self.key = key\n",
    "        self.interaction = interaction\n",
    "        self.tracks = tracks\n",
    "        self.dataframe = None\n",
    "        \n",
    "        self.evaluate()\n",
    "    \n",
    "    def evaluate_for(self, fold):\n",
    "        dataset = Dataset(self.input, fold=fold, tracks=self.tracks)\n",
    "        \n",
    "        train = self.train_builder(dataset.train, self.tracks)\n",
    "        test = self.test_builder(dataset.test, train.dataframe, self.key, self.interaction)\n",
    "        evaluation = Evaluation(dataset.validation, test.dataframe, self.key, fold)\n",
    "        \n",
    "        return evaluation.dataframe\n",
    "    \n",
    "    def evaluate(self):\n",
    "        all = reduce(DataFrame.unionAll, [self.evaluate_for(i + 1) for i in range(self.folds)])\n",
    "        all = (all.union(all\n",
    "            .agg(\n",
    "                avg(col(\"R-precision\")).alias(\"R-precision\"))\n",
    "            .withColumn(\"fold\", lit(\"avg\"))))\n",
    "            \n",
    "        self.dataframe = all\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class HybridCrossValidation:\n",
    "    def __init__(self, input_playlists, input_tracks, folds, train_builder_playlists, train_builder_tracks, test_builder):\n",
    "        self.input_playlists = input_playlists\n",
    "        self.input_tracks = input_tracks\n",
    "        self.folds = folds\n",
    "        self.train_builder_playlists = train_builder_playlists\n",
    "        self.train_builder_tracks = train_builder_tracks\n",
    "        self.test_builder = test_builder\n",
    "        self.dataframe = None\n",
    "        \n",
    "        self.evaluate()\n",
    "    \n",
    "    def evaluate_for(self, fold):\n",
    "        playlist_dataset = Dataset(self.input_playlists, fold=fold, tracks=self.input_tracks)\n",
    "        hybrid_dataset = Dataset(self.input_tracks, fold=fold)\n",
    "         \n",
    "        train_for_playlist = self.train_builder_playlists(playlist_dataset.train, self.input_tracks)\n",
    "        train_for_track = self.train_builder_tracks(hybrid_dataset.train)\n",
    "        \n",
    "        test = self.test_builder(\n",
    "            hybrid_dataset.test, \n",
    "            train_for_playlist.dataframe, \n",
    "            train_for_track.dataframe,\n",
    "            [0.5, 0.5])\n",
    "        \n",
    "        evaluation = Evaluation(hybrid_dataset.validation, test.dataframe, fold=fold)\n",
    "        \n",
    "        return evaluation.dataframe\n",
    "    \n",
    "    def evaluate(self):\n",
    "        all = reduce(DataFrame.unionAll, [self.evaluate_for(i + 1) for i in range(self.folds)])\n",
    "        all = (all.union(all\n",
    "            .agg(\n",
    "                avg(col(\"R-precision\")).alias(\"R-precision\"))\n",
    "            .withColumn(\"fold\", lit(\"avg\"))))\n",
    "            \n",
    "        self.dataframe = all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "cross_validation = CrossValidation(\n",
    "    tracks.dataframe, \n",
    "    5, \n",
    "    Random, \n",
    "    RandomRecommendations,\n",
    "    key=\"playlist_id\")\n",
    "    \n",
    "    \n",
    "\n",
    "z.show(cross_validation.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "playlist_cross_validation = CrossValidation(\n",
    "    playlist_preprocessing.dataframe, \n",
    "    5, \n",
    "    PlaylistSimilaritiesWrapper, \n",
    "    Recommendations,\n",
    "    key=\"playlist_name\",\n",
    "    interaction=\"playlist_name\",\n",
    "    tracks=tracks.dataframe)\n",
    "\n",
    "z.show(playlist_cross_validation.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "track_cross_validation = CrossValidation(\n",
    "    tracks.dataframe, \n",
    "    4, \n",
    "    TrackSimilarities, \n",
    "    Recommendations,\n",
    "    key=\"playlist_id\",\n",
    "    interaction=\"track_uri\")\n",
    "\n",
    "z.show(track_cross_validation.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "track_cross_validation_cosine = CrossValidation(\n",
    "    tracks.dataframe, \n",
    "    4, \n",
    "    TrackSimilaritiesCosine, \n",
    "    Recommendations,\n",
    "    key=\"playlist_id\",\n",
    "    interaction=\"track_uri\")\n",
    "\n",
    "z.show(track_cross_validation_cosine.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "hybrid_cross_validation = HybridCrossValidation(\n",
    "    playlist_preprocessing.dataframe,\n",
    "    track_preprocessing.dataframe,\n",
    "    5,\n",
    "    PlaylistSimilaritiesWrapper, \n",
    "    TrackSimilarities, \n",
    "    HybridRecommendations)\n",
    "\n",
    "z.show(hybrid_cross_validation.dataframe)"
   ]
  }
 ]
}
